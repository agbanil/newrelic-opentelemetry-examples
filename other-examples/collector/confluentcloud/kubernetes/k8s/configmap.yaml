apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-confluent-config
  labels:
    app.kubernetes.io/name: otel-confluent-config
    app.kubernetes.io/part-of: otel-confluent
    component: otel-confluent-config
data:
  otel-confluent-config: |
    receivers:
      kafkametrics:
        brokers: 
          - ${BOOTSTRAP_SERVER}
        protocol_version: 2.0.0
        scrapers:
          - brokers
          - topics
          - consumers
        auth:
          sasl:
            username: ${CONFLUENT_API_KEY}
            password: ${CONFLUENT_API_SECRET}
            mechanism: PLAIN
          tls:
            insecure_skip_verify: false
        collection_interval: 5s
    
      prometheus:
        config:
          scrape_configs:
            - job_name: "confluent"
              scrape_interval: 60s # Do not go any lower than this or you'll hit rate limits
              static_configs:
                - targets: ["api.telemetry.confluent.cloud"]
              scheme: https
              basic_auth:
                username: ${CONFLUENT_API_KEY}
                password: ${CONFLUENT_API_SECRET}
              metrics_path: /v2/metrics/cloud/export
              params:
                "resource.kafka.id":
                  - ${CLUSTER_ID}
    exporters:
      otlp:
        endpoint: ${NEW_RELIC_OTLP_ENDPOINT}
        tls:
          insecure: false
        sending_queue:
          num_consumers: 4
          queue_size: 100
        retry_on_failure:
          enabled: true
        headers:
          api-key: ${NEW_RELIC_API_KEY}
      logging:
        loglevel: debug
    processors:
      batch: # Turn this on if the need arises
        # send_batch_size: 1000
        # send_batch_max_size: 1500
        # timeout: 1s
      memory_limiter: # doc here: https://github.com/open-telemetry/opentelemetry-collector/tree/main/processor/memorylimiterprocessor
        limit_percentage: 80 # Maximum amount of total memory targeted to be allocated by the process heap
        spike_limit_percentage: 30 # Maximum spike expected between the measurements of memory usage 
        check_interval: 5s # Time between measurements of memory usage
      resource:
        attributes: # This is necessary since confluent exposes only the cluster id in the metrics
        - key: kafka.cluster_name
          value: ${CLUSTER_NAME}
          action: upsert
    service:
      pipelines:
        metrics:
          receivers: [prometheus, kafkametrics]
          processors: [memory_limiter, resource, batch]
          exporters: [logging, otlp]
